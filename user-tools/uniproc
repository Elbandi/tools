#!/usr/bin/env perl

=pod

=head1 NAME

uniproc - Universal data processing tool

=head1 SYNOPSIS

uniproc [I<OPTIONS>] I<INPUTFILE> I<COMMAND> [I<ARGS>]

=head1 DESCRIPTION

Take each line from I<INPUTFILE> as B<DATA>,
pass each piece of B<DATA> to I<COMMAND> as the last argument,
then record the exit status.

Can be well parallelized.
uniproc(1) itself does not run multiple instances of I<COMMAND> in parallel, just in series,
but if you start multiple instances of uniproc(1), then you can run I<COMMAND>s concurrently.
Locking ensures no overlapping data being processes.

Use a wrapper command/script for I<COMMAND> if you want either of these:

=over 4

=item save I<COMMAND>'s output as well.

By default it goes to STDOUT.
See redirexec(1) for example.

=item pass B<DATA> on the STDIN or in environment variable instead of command argument.

See args2env(1) for example.

=back

If re-run after an interrupt, won't process already processed data.
But you may re-try the failed ones by B<--retry> option.

The user may append new lines of data to I<INPUTFILE> between executions or during runtime,
but not edit or reorder old ones, otherwise results get confused.

=head1 OPTIONS

=over 4

=item --retry

Process those data which were previously failed (according to B<< I<INPUTFILE>.uniproc >> state file) too,
besides the unprocessed ones.

=item -1, --one-shot

Process only 1 item, then exit.
Default is to process as many items in series as possible.

=back

=head1 FILES

It maintains I<INPUTFILE>.uniproc file
by writing the processing status of each lines of input data in it line-by-line.
Processing status is either:

=over 4

=item all spaces (   )

processing not yet started

=item periods (...)

in progress

=item digits, possibly padded by spaces (  0)

result status (exit code)

=item exclamation mark (C<!>) followed by hexadecimal digits (!0f)

termination signal (I<COMMAND> teminated abnormally)

=item EOF (ie. fewer lines than input data)

processing not yet started

=back

I<INPUTFILE>.uniproc is locked while read/write to ensure consistency.
I<INPUTFILE>.uniproc.B<NUM> are the name of the files which hold the lock for the currently in-progress processes,
where B<NUM> is the line number of the corresponding piece of data in I<INPUTFILE>.
A lock is held on each of these I<INPUTFILE>.proc.B<NUM> files by the respective instance of I<COMMAND>
to detect if the processing is still going or the process crashed.

=head1 LIMITATION

Due to currently used locking mechanism (Fcntl(3perl)), running on multiple hosts may void locking.

=head1 ENVIRONMENT

When running I<COMMAND>, the following environment is set:

=over 4

=item UNIPROC_DATANUM

Number of the particular piece of data (ie. line number in I<INPUTFILE>)
which is need to be processed by the current process.

=back

=head1 EXAMPLES

Display the data processing status before each line of data:

  paste datafile.uniproc datafile

How much competed?

  awk -v total=$(wc -l < datafile) 'BEGIN{ok=ip=fail=0} {if($1==0){ok++} else if($1=="..."){ip++} else if($1!=""){fail++}} END{print "total: "total", completed: "ok" ("(ok*100/total)"%), in-progress: "ip" ("(ip*100/total)"%), failed: "fail" ("(fail*100/total)"%)"}' datafile.uniproc
  
Output:

  total: 8, completed: 4 (50%), in-progress: 1 (12.5%), failed: 1 (12.5%)

Record output of data processing into a file per each piece of data:

  uniproc datafile sh -c 'some-command "$@" | tee output-$UNIPROC_DATANUM' --

  uniproc datafile substenv -e UNIPROC_DATANUM redirexec '1:a:file:output-$UNIPROC_DATANUM' some-command

Display data number, processing status, input data, (last line of) output data in a table:

  join -t $'\t' <(nl -ba -v0 datafile.uniproc) <(nl -ba -v0 datafile) | foreach -t --prefix-add-data --prefix-add-tab tail -n1 output-{0}

=cut


use Fcntl qw/:flock :seek/;
use Data::Dumper;
use Getopt::Long qw/:config no_ignore_case no_bundling no_getopt_compat no_auto_abbrev require_order/;
use IPC::Run qw/run/;
use POSIX;
use Pod::Usage;

$OptRetry = 0;
$OptDebug = 0;
$OptShots = undef;

GetOptions(
	'retry' => \$OptRetry,
	'1|oneshot|one-shot' => sub { $OptShots = 1; },
	'debug' => \$OptDebug,
	'help' => sub { pod2usage(-exitval=>0, -verbose=>99); },
) or pod2usage(-exitval=>2, -verbose=>99);

pod2usage(-exitval=>2, -verbose=>99) unless scalar @ARGV >= 2;


sub debug_msg
{
	warn @_ if $OptDebug;
}

sub deriv_processing_status
{
	my $child_status = shift;
	my $status = sprintf '%3d', WEXITSTATUS($child_status);
	$status = sprintf '!%02x', WTERMSIG($child_status) if WIFSIGNALED($child_status);
	return $status;
}

sub fopen
{
	my $path = shift;
	my $opts = shift;  # supported opts: rw, no_create, lock
	my $mode = '<';  # default mode is read-only, no-create
	if($opts->{'rw'})
	{
		if(not $opts->{'no_create'})
		{
			open my $fh, '>>', $path or die "$0: $path: $!\n";
			close $fh;
		}
		$mode = '+<';
	}
	
	open my $fh, $mode, $path or die "$0: $path: $!\n";
	seek $fh, 0, SEEK_SET or die "$0: seek: $path: $!\n";
	if($opts->{'lock'})
	{
		flock $fh, LOCK_EX or die "$0: flock: $path: $!\n";
	}
	return $fh;
}

sub extend_resultsfile
{
	my $fname = $ResultsFile;
	my $fh = shift;
	my $extended_size = shift;
	
	seek $fh, 0, SEEK_END;
	my $size = tell $fh;
	my $endpos_last_complete_record = int($size / $ResultsRecordLength) * $ResultsRecordLength;
	my $records_to_append = ($extended_size - $endpos_last_complete_record) / $ResultsRecordLength;
	debug_msg "go to offset $endpos_last_complete_record to extend by $records_to_append records\n";
	seek $fh, $size, SEEK_SET;
	# fill up with empty status records
	print {$Results_fh} "   \n" x $records_to_append or die "$0: write: $fname: $!\n";
}

sub record_status
{
	my $linenum = shift;
	my $status = shift;
	die "$0: size mismatch: length(\"$status\") != $ResultsRecordLength - 1\n" if length($status) != $ResultsRecordLength - 1;
	
	my $offset = $linenum * $ResultsRecordLength;
	debug_msg "go to offset $offset to record data # $linenum 's status \"$status\"\n";
	seek $Results_fh, $offset, SEEK_SET;
	if(eof $Results_fh)
	{
		debug_msg "eof\n";
		# results file is not big enough, let's extend
		extend_resultsfile($Results_fh, $offset);
		seek $Results_fh, $offset, SEEK_SET;
	}
	print {$Results_fh} "$status\n" or die "$0: write: $ResultsFile: $!\n";
}

sub input_data
{
	# TODO use index file, if exists, to seek in.
	
	my $asked_line = shift;
	my $fh = fopen $InputFile, {no_create=>1,};
	my $linenum = 0;
	my $data = undef;
	while(my $line = <$fh>)
	{
		if($linenum == $asked_line)
		{
			$data = $line;
			chomp $data;
			last;
		}
		$linenum++;
	}
	close $fh;
	return $data;
}

sub count_input_records
{
	# TODO use index file, if exists.
	
	my $fh = fopen $InputFile, {no_create=>1,};
	my $linenum = 0;
	$linenum++ while scalar <$fh>;
	close $fh;
	return $linenum;
}

sub processing_lockfile_name
{
	my $processing_number = shift;
	return "$InputFile.uniproc.$processing_number";
}

sub still_in_progress
{
	my $processing_number = shift;
	my $lockfile = processing_lockfile_name($processing_number);
	open my $fh, '<', $lockfile or return 0;
	my $lock_ok = flock $fh, LOCK_EX|LOCK_NB;
	close $fh;
	return not $lock_ok;
}

sub get_next_data_number
{
	debug_msg "go to offset ".($FirstUnprocessed*$ResultsRecordLength)." to read status of record # $FirstUnprocessed\n";
	seek $Results_fh, $FirstUnprocessed*$ResultsRecordLength, SEEK_SET;
	
	my $record_num = $FirstUnprocessed;
	my $result;
	while(1)
	{
		my $nbytes = read $Results_fh, $result, $ResultsRecordLength;
		debug_msg "read only $nbytes bytes \"$result\" at record $record_num\n" if $nbytes < $ResultsRecordLength;
		last if $nbytes < $ResultsRecordLength;
		chomp $result;
		
		if($result eq '   ')
		{
			$FirstUnprocessed = $record_num;
			debug_msg "uninitialized $record_num\n";
			return $record_num;
		}
		if($result eq '...')
		{
			# check if still locked
			if(not still_in_progress($record_num))
			{
				debug_msg "crashed $record_num\n";
				return $record_num;
			}
		}
		if($OptRetry and ($result =~ /^!/ or ($result =~ /^\s*\d+$/ and $result > 0)))
		{
			$FirstUnprocessed = $record_num;
			debug_msg "retry $record_num\n";
			return $record_num;
		}
		
		$record_num++;
	}
	
	# check here if there are more input data than result records
	my $input_records = count_input_records();
	if($record_num < $input_records)
	{
		extend_resultsfile($Results_fh, $input_records * $ResultsRecordLength);
		$FirstUnprocessed = $record_num;
		debug_msg "new $record_num\n";
		return $record_num;
	}
	
	# no more input data
	debug_msg "no more input. input_records=$input_records\n";
	return undef;
}


($InputFile, $Command, @Args) = @ARGV;

$FirstUnprocessed = 0;
$ResultsRecordLength = 4;
$ResultsFile = "$InputFile.uniproc";
$Results_fh = undef;
$num_shots = 0;

while(not defined $OptShots or $num_shots < $OptShots)
{
	$Results_fh = fopen $ResultsFile, {rw=>1, lock=>1};
	my $LineNum = get_next_data_number();
	last if not defined $LineNum;
	
	my $Data = input_data($LineNum);
	
	my $InprogressFile = processing_lockfile_name($LineNum);
	my $inprogress_fh = fopen $InprogressFile, {rw=>1, lock=>1};
	
	record_status($LineNum, '...');
	close $Results_fh;
	
	$ENV{'UNIPROC_DATANUM'} = $LineNum;
	run [$Command, @Args, $Data];
	my $status = $?;
	
	$Results_fh = fopen $ResultsFile, {rw=>1, lock=>1};
	record_status($LineNum, deriv_processing_status($status));
	close $Results_fh;
	
	unlink $InprogressFile;
	close $inprogress_fh;
	
	$num_shots++;
}
